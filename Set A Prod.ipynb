{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f10aa12a-d98c-4c57-be85-9bda9acccc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device for Set A: mps\n",
      "\n",
      "--- Set A: UCI Adult (Classical Baseline) ---\n",
      "Epochs: 50, Device: mps\n",
      "\n",
      "Loading UCI Adult dataset...\n",
      "UCI Adult: train=39073, test=9769, input_dim=97\n",
      "\n",
      "Model A1: Logistic Regression\n",
      "\n",
      "Training model with 196 trainable parameters.\n",
      "Epoch [1/50] - Train Loss: 0.5206\n",
      "Epoch [2/50] - Train Loss: 0.3988\n",
      "Epoch [3/50] - Train Loss: 0.3662\n",
      "Epoch [4/50] - Train Loss: 0.3515\n",
      "Epoch [5/50] - Train Loss: 0.3433\n",
      "Epoch [6/50] - Train Loss: 0.3380\n",
      "Epoch [7/50] - Train Loss: 0.3342\n",
      "Epoch [8/50] - Train Loss: 0.3314\n",
      "Epoch [9/50] - Train Loss: 0.3292\n",
      "Epoch [10/50] - Train Loss: 0.3275\n",
      "Epoch [11/50] - Train Loss: 0.3259\n",
      "Epoch [12/50] - Train Loss: 0.3248\n",
      "Epoch [13/50] - Train Loss: 0.3238\n",
      "Epoch [14/50] - Train Loss: 0.3231\n",
      "Epoch [15/50] - Train Loss: 0.3224\n",
      "Epoch [16/50] - Train Loss: 0.3219\n",
      "Epoch [17/50] - Train Loss: 0.3214\n",
      "Epoch [18/50] - Train Loss: 0.3211\n",
      "Epoch [19/50] - Train Loss: 0.3207\n",
      "Epoch [20/50] - Train Loss: 0.3204\n",
      "Epoch [21/50] - Train Loss: 0.3202\n",
      "Epoch [22/50] - Train Loss: 0.3200\n",
      "Epoch [23/50] - Train Loss: 0.3199\n",
      "Epoch [24/50] - Train Loss: 0.3197\n",
      "Epoch [25/50] - Train Loss: 0.3197\n",
      "Epoch [26/50] - Train Loss: 0.3195\n",
      "Epoch [27/50] - Train Loss: 0.3195\n",
      "Epoch [28/50] - Train Loss: 0.3194\n",
      "Epoch [29/50] - Train Loss: 0.3194\n",
      "Epoch [30/50] - Train Loss: 0.3194\n",
      "Epoch [31/50] - Train Loss: 0.3193\n",
      "Epoch [32/50] - Train Loss: 0.3192\n",
      "Epoch [33/50] - Train Loss: 0.3192\n",
      "Epoch [34/50] - Train Loss: 0.3192\n",
      "Epoch [35/50] - Train Loss: 0.3191\n",
      "Epoch [36/50] - Train Loss: 0.3192\n",
      "Epoch [37/50] - Train Loss: 0.3191\n",
      "Epoch [38/50] - Train Loss: 0.3191\n",
      "Epoch [39/50] - Train Loss: 0.3191\n",
      "Epoch [40/50] - Train Loss: 0.3191\n",
      "Epoch [41/50] - Train Loss: 0.3191\n",
      "Epoch [42/50] - Train Loss: 0.3190\n",
      "Epoch [43/50] - Train Loss: 0.3190\n",
      "Epoch [44/50] - Train Loss: 0.3190\n",
      "Epoch [45/50] - Train Loss: 0.3189\n",
      "Epoch [46/50] - Train Loss: 0.3190\n",
      "Epoch [47/50] - Train Loss: 0.3189\n",
      "Epoch [48/50] - Train Loss: 0.3189\n",
      "Epoch [49/50] - Train Loss: 0.3189\n",
      "Epoch [50/50] - Train Loss: 0.3189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r7/j8qcxj351zv_wpqhpts6z__40000gn/T/ipykernel_59573/773954926.py:62: UserWarning: The operator 'aten::linalg_svd' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:14.)\n",
      "  _, S, _ = torch.linalg.svd(W, full_matrices=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model A2: Shallow FFN (64 hidden units)\n",
      "\n",
      "Training model with 6402 trainable parameters.\n",
      "Epoch [1/50] - Train Loss: 0.4061\n",
      "Epoch [2/50] - Train Loss: 0.3180\n",
      "Epoch [3/50] - Train Loss: 0.3118\n",
      "Epoch [4/50] - Train Loss: 0.3090\n",
      "Epoch [5/50] - Train Loss: 0.3072\n",
      "Epoch [6/50] - Train Loss: 0.3056\n",
      "Epoch [7/50] - Train Loss: 0.3044\n",
      "Epoch [8/50] - Train Loss: 0.3037\n",
      "Epoch [9/50] - Train Loss: 0.3026\n",
      "Epoch [10/50] - Train Loss: 0.3017\n",
      "Epoch [11/50] - Train Loss: 0.3009\n",
      "Epoch [12/50] - Train Loss: 0.3002\n",
      "Epoch [13/50] - Train Loss: 0.2992\n",
      "Epoch [14/50] - Train Loss: 0.2988\n",
      "Epoch [15/50] - Train Loss: 0.2984\n",
      "Epoch [16/50] - Train Loss: 0.2973\n",
      "Epoch [17/50] - Train Loss: 0.2966\n",
      "Epoch [18/50] - Train Loss: 0.2958\n",
      "Epoch [19/50] - Train Loss: 0.2953\n",
      "Epoch [20/50] - Train Loss: 0.2948\n",
      "Epoch [21/50] - Train Loss: 0.2942\n",
      "Epoch [22/50] - Train Loss: 0.2937\n",
      "Epoch [23/50] - Train Loss: 0.2931\n",
      "Epoch [24/50] - Train Loss: 0.2925\n",
      "Epoch [25/50] - Train Loss: 0.2921\n",
      "Epoch [26/50] - Train Loss: 0.2917\n",
      "Epoch [27/50] - Train Loss: 0.2910\n",
      "Epoch [28/50] - Train Loss: 0.2909\n",
      "Epoch [29/50] - Train Loss: 0.2904\n",
      "Epoch [30/50] - Train Loss: 0.2901\n",
      "Epoch [31/50] - Train Loss: 0.2897\n",
      "Epoch [32/50] - Train Loss: 0.2890\n",
      "Epoch [33/50] - Train Loss: 0.2886\n",
      "Epoch [34/50] - Train Loss: 0.2885\n",
      "Epoch [35/50] - Train Loss: 0.2882\n",
      "Epoch [36/50] - Train Loss: 0.2883\n",
      "Epoch [37/50] - Train Loss: 0.2870\n",
      "Epoch [38/50] - Train Loss: 0.2867\n",
      "Epoch [39/50] - Train Loss: 0.2866\n",
      "Epoch [40/50] - Train Loss: 0.2864\n",
      "Epoch [41/50] - Train Loss: 0.2864\n",
      "Epoch [42/50] - Train Loss: 0.2854\n",
      "Epoch [43/50] - Train Loss: 0.2850\n",
      "Epoch [44/50] - Train Loss: 0.2850\n",
      "Epoch [45/50] - Train Loss: 0.2850\n",
      "Epoch [46/50] - Train Loss: 0.2850\n",
      "Epoch [47/50] - Train Loss: 0.2840\n",
      "Epoch [48/50] - Train Loss: 0.2834\n",
      "Epoch [49/50] - Train Loss: 0.2833\n",
      "Epoch [50/50] - Train Loss: 0.2831\n",
      "\n",
      "Set A results saved to 'dissertation_results_set_a.csv'\n",
      "                id  params  train_error  test_error   gen_gap    l2_norm  \\\n",
      "0         A_LogReg     196     0.147826    0.146791 -0.001035   5.480762   \n",
      "1  A_ShallowFFN_64    6402     0.130934    0.142594  0.011660  17.259153   \n",
      "\n",
      "   spectral_norm  sharpness  \n",
      "0       5.457900   0.000424  \n",
      "1       9.567402   0.002149  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "\n",
    "# ----------------------------------\n",
    "# 0. Reproducibility and device\n",
    "# ----------------------------------\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"Set random seeds for reproducible experiments.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"Selects MPS (Apple), CUDA, or CPU.\"\"\"\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "set_seed(42)\n",
    "DEVICE = get_device()\n",
    "print(\"Using device for Set A:\", DEVICE)\n",
    "\n",
    "EPOCHS_A = 50\n",
    "LR_A = 1e-3\n",
    "BATCH_SIZE_A = 256\n",
    "\n",
    "# ----------------------------------\n",
    "# 1. Complexity measure helpers\n",
    "# ----------------------------------\n",
    "\n",
    "def calculate_l2_norm(model: nn.Module) -> float:\n",
    "    \"\"\"Computes the Frobenius norm of all weight matrices.\"\"\"\n",
    "    l2_norm = 0.0\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            l2_norm += torch.sum(param.detach() ** 2)\n",
    "    return torch.sqrt(l2_norm).item()\n",
    "\n",
    "def calculate_spectral_norm(model: nn.Module) -> float:\n",
    "    \"\"\"Computes the sum of maximum singular values across weight matrices.\"\"\"\n",
    "    spectral_norm_sum = 0.0\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name and param.dim() > 1:\n",
    "            W = param\n",
    "            try:\n",
    "                if W.numel() > 0:\n",
    "                    _, S, _ = torch.linalg.svd(W, full_matrices=False)\n",
    "                    spectral_norm_sum += S[0].item()\n",
    "            except Exception:\n",
    "                continue\n",
    "    return spectral_norm_sum\n",
    "\n",
    "def calculate_sharpness(\n",
    "    model: nn.Module,\n",
    "    criterion: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    rho: float = 0.01,\n",
    "    device: torch.device = DEVICE,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Approximates sharpness using a single SAM-style perturbation step.\n",
    "    S(w*) = (L(w* + Îµ) - L(w*)) / (1 + L(w*)).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    try:\n",
    "        data_batch, target_batch = next(iter(data_loader))\n",
    "    except StopIteration:\n",
    "        return 0.0\n",
    "\n",
    "    data_batch, target_batch = data_batch.to(device), target_batch.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(data_batch)\n",
    "        base_loss = criterion(out, target_batch).item()\n",
    "\n",
    "    model.zero_grad()\n",
    "    out = model(data_batch)\n",
    "    loss = criterion(out, target_batch)\n",
    "    loss.backward()\n",
    "\n",
    "    grad_norm_sq = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            grad_norm_sq += torch.sum(p.grad ** 2)\n",
    "    grad_norm = torch.sqrt(grad_norm_sq)\n",
    "    if grad_norm.item() == 0.0:\n",
    "        return 0.0\n",
    "\n",
    "    epsilon_map = {}\n",
    "    for name, p in model.named_parameters():\n",
    "        if p.grad is not None:\n",
    "            eps = (p.grad / grad_norm) * rho\n",
    "            p.data.add_(eps)\n",
    "            epsilon_map[name] = eps\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_pert = model(data_batch)\n",
    "        pert_loss = criterion(out_pert, target_batch).item()\n",
    "\n",
    "    for name, p in model.named_parameters():\n",
    "        if name in epsilon_map:\n",
    "            p.data.sub_(epsilon_map[name])\n",
    "\n",
    "    sharp = (pert_loss - base_loss) / (1.0 + base_loss)\n",
    "    return max(0.0, sharp)\n",
    "\n",
    "def evaluate_model(\n",
    "    model: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device = DEVICE,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Computes average loss and classification error on a dataset.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            correct += (preds == target).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    error = 1.0 - (correct / total)\n",
    "    return avg_loss, error\n",
    "\n",
    "# ----------------------------------\n",
    "# 2. Data loading and preprocessing\n",
    "# ----------------------------------\n",
    "\n",
    "def load_uci_adult(test_size: float = 0.2, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Loads the UCI Adult dataset from OpenML, applies one-hot encoding to\n",
    "    categorical variables and standardisation to numeric features.\n",
    "    \"\"\"\n",
    "    print(\"\\nLoading UCI Adult dataset...\")\n",
    "    adult = fetch_openml(\"adult\", version=2, as_frame=True)\n",
    "    df = adult.frame.copy()\n",
    "\n",
    "    target_col = \"class\"\n",
    "    y = (df[target_col] == \">50K\").astype(int).values\n",
    "    X = df.drop(columns=[target_col])\n",
    "\n",
    "    cat_cols = X.select_dtypes(include=[\"category\", \"object\"]).columns\n",
    "    num_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "\n",
    "    X_cat = pd.get_dummies(X[cat_cols], drop_first=True)\n",
    "    scaler = StandardScaler()\n",
    "    X_num = pd.DataFrame(scaler.fit_transform(X[num_cols]), columns=num_cols)\n",
    "\n",
    "    X_proc = pd.concat([X_num, X_cat], axis=1)\n",
    "    X_array = X_proc.values.astype(np.float32)\n",
    "    y_array = y.astype(np.int64)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_array,\n",
    "        y_array,\n",
    "        test_size=test_size,\n",
    "        random_state=seed,\n",
    "        stratify=y_array,\n",
    "    )\n",
    "\n",
    "    X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "    X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_t = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "    test_ds = TensorDataset(X_test_t, y_test_t)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE_A, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE_A, shuffle=False)\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "    print(f\"UCI Adult: train={X_train.shape[0]}, test={X_test.shape[0]}, input_dim={input_dim}\")\n",
    "    return train_loader, test_loader, input_dim\n",
    "\n",
    "# ----------------------------------\n",
    "# 3. Model definitions (Set A)\n",
    "# ----------------------------------\n",
    "\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    \"\"\"Single linear layer for binary classification.\"\"\"\n",
    "    def __init__(self, input_dim: int, num_classes: int = 2):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(x)\n",
    "\n",
    "class ShallowFFN(nn.Module):\n",
    "    \"\"\"One-hidden-layer feed-forward network.\"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int = 64, num_classes: int = 2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# ----------------------------------\n",
    "# 4. Training and evaluation wrapper\n",
    "# ----------------------------------\n",
    "\n",
    "def train_and_evaluate_tabular(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    test_loader: DataLoader,\n",
    "    epochs: int = EPOCHS_A,\n",
    "    lr: float = LR_A,\n",
    "    device: torch.device = DEVICE,\n",
    ") -> dict:\n",
    "    \"\"\"Trains a tabular model and returns generalisation and complexity metrics.\"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nTraining model with {total_params} trainable parameters.\")\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = criterion(out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * data.size(0)\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    train_loss, train_error = evaluate_model(model, train_loader, criterion, device)\n",
    "    test_loss, test_error = evaluate_model(model, test_loader, criterion, device)\n",
    "    gen_gap = test_error - train_error\n",
    "\n",
    "    l2 = calculate_l2_norm(model)\n",
    "    spec = calculate_spectral_norm(model)\n",
    "    sharp = calculate_sharpness(model, criterion, train_loader, device=device)\n",
    "\n",
    "    return {\n",
    "        \"params\": total_params,\n",
    "        \"train_error\": train_error,\n",
    "        \"test_error\": test_error,\n",
    "        \"gen_gap\": gen_gap,\n",
    "        \"l2_norm\": l2,\n",
    "        \"spectral_norm\": spec,\n",
    "        \"sharpness\": sharp,\n",
    "    }\n",
    "\n",
    "# ----------------------------------\n",
    "# 5. Main entry point (Set A)\n",
    "# ----------------------------------\n",
    "\n",
    "def run_set_a_experiment():\n",
    "    \"\"\"Runs Set A experiments on UCI Adult with logistic regression and shallow FFN.\"\"\"\n",
    "    print(\"\\n--- Set A: UCI Adult (Classical Baseline) ---\")\n",
    "    print(f\"Epochs: {EPOCHS_A}, Device: {DEVICE}\")\n",
    "\n",
    "    train_loader, test_loader, input_dim = load_uci_adult(test_size=0.2, seed=42)\n",
    "    results = []\n",
    "\n",
    "    print(\"\\nModel A1: Logistic Regression\")\n",
    "    logreg = LogisticRegressionModel(input_dim=input_dim, num_classes=2)\n",
    "    m_logreg = train_and_evaluate_tabular(logreg, train_loader, test_loader)\n",
    "    m_logreg[\"id\"] = \"A_LogReg\"\n",
    "    results.append(m_logreg)\n",
    "\n",
    "    print(\"\\nModel A2: Shallow FFN (64 hidden units)\")\n",
    "    shallow = ShallowFFN(input_dim=input_dim, hidden_dim=64, num_classes=2)\n",
    "    m_ffn = train_and_evaluate_tabular(shallow, train_loader, test_loader)\n",
    "    m_ffn[\"id\"] = \"A_ShallowFFN_64\"\n",
    "    results.append(m_ffn)\n",
    "\n",
    "    df_a = pd.DataFrame(results)\n",
    "    out_name = \"dissertation_results_set_a.csv\"\n",
    "    df_a.to_csv(out_name, index=False)\n",
    "    print(f\"\\nSet A results saved to '{out_name}'\")\n",
    "    print(df_a[[\"id\", \"params\", \"train_error\", \"test_error\", \"gen_gap\",\n",
    "                \"l2_norm\", \"spectral_norm\", \"sharpness\"]])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_set_a_experiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff353e5-6d83-4da5-a8ee-90c161e0d6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
