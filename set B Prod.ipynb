{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2e07e7f-00f9-4fca-96f5-7084bcf8f9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device for Set B: mps\n",
      "\n",
      "--- Set B: MNIST with 25% Label Noise (Memorisation Regime) ---\n",
      "Epochs: 100, Batch size: 128, Device: mps\n",
      "Total trainable parameters (Set B model): 1457674\n",
      "Epoch [1/100] - Train Loss: 1.8620\n",
      "Epoch [2/100] - Train Loss: 1.3144\n",
      "Epoch [3/100] - Train Loss: 1.2510\n",
      "Epoch [4/100] - Train Loss: 1.2184\n",
      "Epoch [5/100] - Train Loss: 1.1984\n",
      "Epoch [6/100] - Train Loss: 1.1848\n",
      "Epoch [7/100] - Train Loss: 1.1733\n",
      "Epoch [8/100] - Train Loss: 1.1641\n",
      "Epoch [9/100] - Train Loss: 1.1567\n",
      "Epoch [10/100] - Train Loss: 1.1495\n",
      "Epoch [11/100] - Train Loss: 1.1433\n",
      "Epoch [12/100] - Train Loss: 1.1358\n",
      "Epoch [13/100] - Train Loss: 1.1289\n",
      "Epoch [14/100] - Train Loss: 1.1236\n",
      "Epoch [15/100] - Train Loss: 1.1163\n",
      "Epoch [16/100] - Train Loss: 1.1112\n",
      "Epoch [17/100] - Train Loss: 1.1056\n",
      "Epoch [18/100] - Train Loss: 1.0994\n",
      "Epoch [19/100] - Train Loss: 1.0928\n",
      "Epoch [20/100] - Train Loss: 1.0847\n",
      "Epoch [21/100] - Train Loss: 1.0766\n",
      "Epoch [22/100] - Train Loss: 1.0703\n",
      "Epoch [23/100] - Train Loss: 1.0637\n",
      "Epoch [24/100] - Train Loss: 1.0542\n",
      "Epoch [25/100] - Train Loss: 1.0471\n",
      "Epoch [26/100] - Train Loss: 1.0387\n",
      "Epoch [27/100] - Train Loss: 1.0269\n",
      "Epoch [28/100] - Train Loss: 1.0135\n",
      "Epoch [29/100] - Train Loss: 1.0050\n",
      "Epoch [30/100] - Train Loss: 0.9935\n",
      "Epoch [31/100] - Train Loss: 0.9789\n",
      "Epoch [32/100] - Train Loss: 0.9666\n",
      "Epoch [33/100] - Train Loss: 0.9566\n",
      "Epoch [34/100] - Train Loss: 0.9383\n",
      "Epoch [35/100] - Train Loss: 0.9264\n",
      "Epoch [36/100] - Train Loss: 0.9064\n",
      "Epoch [37/100] - Train Loss: 0.8847\n",
      "Epoch [38/100] - Train Loss: 0.8727\n",
      "Epoch [39/100] - Train Loss: 0.8502\n",
      "Epoch [40/100] - Train Loss: 0.8338\n",
      "Epoch [41/100] - Train Loss: 0.8135\n",
      "Epoch [42/100] - Train Loss: 0.7888\n",
      "Epoch [43/100] - Train Loss: 0.7618\n",
      "Epoch [44/100] - Train Loss: 0.7400\n",
      "Epoch [45/100] - Train Loss: 0.7229\n",
      "Epoch [46/100] - Train Loss: 0.6938\n",
      "Epoch [47/100] - Train Loss: 0.6686\n",
      "Epoch [48/100] - Train Loss: 0.6444\n",
      "Epoch [49/100] - Train Loss: 0.6145\n",
      "Epoch [50/100] - Train Loss: 0.5900\n",
      "Epoch [51/100] - Train Loss: 0.5624\n",
      "Epoch [52/100] - Train Loss: 0.5383\n",
      "Epoch [53/100] - Train Loss: 0.5127\n",
      "Epoch [54/100] - Train Loss: 0.4856\n",
      "Epoch [55/100] - Train Loss: 0.4624\n",
      "Epoch [56/100] - Train Loss: 0.4392\n",
      "Epoch [57/100] - Train Loss: 0.4044\n",
      "Epoch [58/100] - Train Loss: 0.3795\n",
      "Epoch [59/100] - Train Loss: 0.3451\n",
      "Epoch [60/100] - Train Loss: 0.3200\n",
      "Epoch [61/100] - Train Loss: 0.3032\n",
      "Epoch [62/100] - Train Loss: 0.2773\n",
      "Epoch [63/100] - Train Loss: 0.2475\n",
      "Epoch [64/100] - Train Loss: 0.2312\n",
      "Epoch [65/100] - Train Loss: 0.2099\n",
      "Epoch [66/100] - Train Loss: 0.1893\n",
      "Epoch [67/100] - Train Loss: 0.1748\n",
      "Epoch [68/100] - Train Loss: 0.1596\n",
      "Epoch [69/100] - Train Loss: 0.1463\n",
      "Epoch [70/100] - Train Loss: 0.1359\n",
      "Epoch [71/100] - Train Loss: 0.1262\n",
      "Epoch [72/100] - Train Loss: 0.1186\n",
      "Epoch [73/100] - Train Loss: 0.1137\n",
      "Epoch [74/100] - Train Loss: 0.1078\n",
      "Epoch [75/100] - Train Loss: 0.1035\n",
      "Epoch [76/100] - Train Loss: 0.0999\n",
      "Epoch [77/100] - Train Loss: 0.0966\n",
      "Epoch [78/100] - Train Loss: 0.0933\n",
      "Epoch [79/100] - Train Loss: 0.0900\n",
      "Epoch [80/100] - Train Loss: 0.0874\n",
      "Epoch [81/100] - Train Loss: 0.0846\n",
      "Epoch [82/100] - Train Loss: 0.0821\n",
      "Epoch [83/100] - Train Loss: 0.0799\n",
      "Epoch [84/100] - Train Loss: 0.0777\n",
      "Epoch [85/100] - Train Loss: 0.0754\n",
      "Epoch [86/100] - Train Loss: 0.0741\n",
      "Epoch [87/100] - Train Loss: 0.0725\n",
      "Epoch [88/100] - Train Loss: 0.0712\n",
      "Epoch [89/100] - Train Loss: 0.0695\n",
      "Epoch [90/100] - Train Loss: 0.0682\n",
      "Epoch [91/100] - Train Loss: 0.0671\n",
      "Epoch [92/100] - Train Loss: 0.0660\n",
      "Epoch [93/100] - Train Loss: 0.0652\n",
      "Epoch [94/100] - Train Loss: 0.0645\n",
      "Epoch [95/100] - Train Loss: 0.0638\n",
      "Epoch [96/100] - Train Loss: 0.0633\n",
      "Epoch [97/100] - Train Loss: 0.0628\n",
      "Epoch [98/100] - Train Loss: 0.0625\n",
      "Epoch [99/100] - Train Loss: 0.0622\n",
      "Epoch [100/100] - Train Loss: 0.0621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r7/j8qcxj351zv_wpqhpts6z__40000gn/T/ipykernel_59661/3364249274.py:61: UserWarning: The operator 'aten::linalg_svd' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:14.)\n",
      "  _, S, _ = torch.linalg.svd(W, full_matrices=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Set B results saved to 'dissertation_results_set_b.csv'\n",
      "            id   params  train_error  test_error   gen_gap    l2_norm  \\\n",
      "0  B_FFN_512x5  1457674     0.017217      0.1612  0.143983  47.206608   \n",
      "\n",
      "   spectral_norm  sharpness  \n",
      "0      21.608106   0.022762  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# ----------------------------------\n",
    "# 0. Reproducibility and device\n",
    "# ----------------------------------\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"Set random seeds for reproducible experiments.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"Selects MPS (Apple), CUDA, or CPU.\"\"\"\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "set_seed(42)\n",
    "DEVICE = get_device()\n",
    "print(\"Using device for Set B:\", DEVICE)\n",
    "\n",
    "EPOCHS_B = 100\n",
    "LR_B = 0.01\n",
    "BATCH_SIZE_B = 128\n",
    "NOISE_FRACTION = 0.25\n",
    "\n",
    "# ----------------------------------\n",
    "# 1. Complexity measure helpers\n",
    "# ----------------------------------\n",
    "\n",
    "def calculate_l2_norm(model: nn.Module) -> float:\n",
    "    \"\"\"Computes the Frobenius norm of all weight matrices.\"\"\"\n",
    "    l2_norm = 0.0\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            l2_norm += torch.sum(param.detach() ** 2)\n",
    "    return torch.sqrt(l2_norm).item()\n",
    "\n",
    "def calculate_spectral_norm(model: nn.Module) -> float:\n",
    "    \"\"\"Computes the sum of maximum singular values across weight matrices.\"\"\"\n",
    "    spectral_norm_sum = 0.0\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name and param.dim() > 1:\n",
    "            W = param\n",
    "            try:\n",
    "                if W.numel() > 0:\n",
    "                    _, S, _ = torch.linalg.svd(W, full_matrices=False)\n",
    "                    spectral_norm_sum += S[0].item()\n",
    "            except Exception:\n",
    "                continue\n",
    "    return spectral_norm_sum\n",
    "\n",
    "def calculate_sharpness(\n",
    "    model: nn.Module,\n",
    "    criterion: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    rho: float = 0.01,\n",
    "    device: torch.device = DEVICE,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Approximates sharpness using a single SAM-style perturbation step.\n",
    "    S(w*) = (L(w* + Îµ) - L(w*)) / (1 + L(w*)).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    try:\n",
    "        data_batch, target_batch = next(iter(data_loader))\n",
    "    except StopIteration:\n",
    "        return 0.0\n",
    "\n",
    "    data_batch, target_batch = data_batch.to(device), target_batch.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(data_batch)\n",
    "        base_loss = criterion(outputs, target_batch).item()\n",
    "\n",
    "    model.zero_grad()\n",
    "    outputs = model(data_batch)\n",
    "    loss = criterion(outputs, target_batch)\n",
    "    loss.backward()\n",
    "\n",
    "    grad_norm_sq = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            grad_norm_sq += torch.sum(p.grad ** 2)\n",
    "    grad_norm = torch.sqrt(grad_norm_sq)\n",
    "    if grad_norm.item() == 0.0:\n",
    "        return 0.0\n",
    "\n",
    "    epsilon_map = {}\n",
    "    for name, p in model.named_parameters():\n",
    "        if p.grad is not None:\n",
    "            eps = (p.grad / grad_norm) * rho\n",
    "            p.data.add_(eps)\n",
    "            epsilon_map[name] = eps\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs_perturbed = model(data_batch)\n",
    "        pert_loss = criterion(outputs_perturbed, target_batch).item()\n",
    "\n",
    "    for name, p in model.named_parameters():\n",
    "        if name in epsilon_map:\n",
    "            p.data.sub_(epsilon_map[name])\n",
    "\n",
    "    sharp = (pert_loss - base_loss) / (1.0 + base_loss)\n",
    "    return max(0.0, sharp)\n",
    "\n",
    "def evaluate_model(\n",
    "    model: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device = DEVICE,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Computes average loss and classification error on a dataset.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            correct += (preds == target).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    error = 1.0 - (correct / total)\n",
    "    return avg_loss, error\n",
    "\n",
    "# ----------------------------------\n",
    "# 2. Model definition (Deep FFN)\n",
    "# ----------------------------------\n",
    "\n",
    "class DeepFFN(nn.Module):\n",
    "    \"\"\"Five-layer fully-connected network for the memorisation regime.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = 784,\n",
    "        hidden_dim: int = 512,\n",
    "        num_hidden_layers: int = 5,\n",
    "        num_classes: int = 10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        layers.append(nn.Linear(hidden_dim, num_classes))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.net(x)\n",
    "\n",
    "# ----------------------------------\n",
    "# 3. Data loading with label noise\n",
    "# ----------------------------------\n",
    "\n",
    "def load_mnist_with_label_noise(\n",
    "    batch_size: int = BATCH_SIZE_B,\n",
    "    noise_fraction: float = NOISE_FRACTION,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads the MNIST dataset and applies label corruption to a fraction of the\n",
    "    training labels. The test set remains unchanged.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    full_train = datasets.MNIST(\"./data\", train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST(\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "    train_dataset = full_train\n",
    "\n",
    "    targets = train_dataset.targets.clone()\n",
    "    n_train = len(train_dataset)\n",
    "    n_noisy = int(noise_fraction * n_train)\n",
    "\n",
    "    rng = torch.Generator().manual_seed(seed)\n",
    "    noisy_indices = torch.randperm(n_train, generator=rng)[:n_noisy]\n",
    "\n",
    "    for idx in noisy_indices:\n",
    "        original_label = targets[idx].item()\n",
    "        new_label = random.randint(0, 9)\n",
    "        while new_label == original_label:\n",
    "            new_label = random.randint(0, 9)\n",
    "        targets[idx] = new_label\n",
    "\n",
    "    train_dataset.targets = targets\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# ----------------------------------\n",
    "# 4. Training and experiment driver\n",
    "# ----------------------------------\n",
    "\n",
    "def train_and_evaluate_set_b(\n",
    "    epochs: int = EPOCHS_B,\n",
    "    lr: float = LR_B,\n",
    "    batch_size: int = BATCH_SIZE_B,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Trains a deep FFN on MNIST with 25% randomised labels and reports\n",
    "    generalisation and complexity metrics.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Set B: MNIST with 25% Label Noise (Memorisation Regime) ---\")\n",
    "    print(f\"Epochs: {epochs}, Batch size: {batch_size}, Device: {DEVICE}\")\n",
    "\n",
    "    train_loader, test_loader = load_mnist_with_label_noise(\n",
    "        batch_size=batch_size,\n",
    "        noise_fraction=NOISE_FRACTION,\n",
    "        seed=42,\n",
    "    )\n",
    "\n",
    "    model = DeepFFN().to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total trainable parameters (Set B model): {total_params}\")\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * data.size(0)\n",
    "\n",
    "        scheduler.step()\n",
    "        avg_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    train_loss, train_error = evaluate_model(model, train_loader, criterion, DEVICE)\n",
    "    test_loss, test_error = evaluate_model(model, test_loader, criterion, DEVICE)\n",
    "    gen_gap = test_error - train_error\n",
    "\n",
    "    l2 = calculate_l2_norm(model)\n",
    "    spec = calculate_spectral_norm(model)\n",
    "    sharp = calculate_sharpness(model, criterion, train_loader, device=DEVICE)\n",
    "\n",
    "    result = {\n",
    "        \"id\": \"B_FFN_512x5\",\n",
    "        \"params\": total_params,\n",
    "        \"train_error\": train_error,\n",
    "        \"test_error\": test_error,\n",
    "        \"gen_gap\": gen_gap,\n",
    "        \"l2_norm\": l2,\n",
    "        \"spectral_norm\": spec,\n",
    "        \"sharpness\": sharp,\n",
    "    }\n",
    "\n",
    "    df_b = pd.DataFrame([result])\n",
    "    out_name = \"dissertation_results_set_b.csv\"\n",
    "    df_b.to_csv(out_name, index=False)\n",
    "    print(f\"\\nSet B results saved to '{out_name}'\")\n",
    "    print(df_b)\n",
    "\n",
    "    return df_b\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_and_evaluate_set_b()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd655ba-734e-42b5-9b5a-9ea80d757a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
